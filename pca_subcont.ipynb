{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *setup_dataset* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataset Zan produced \n",
    "# two fields from Alicia’s metadata + Julia's sample QC metadata + variant QC metadata + Konrad’s densified mt\n",
    "mt = hl.read_matrix_table('gs://african-seq-data/hgdp_tgp/hgdp_tgp_dense_meta_filt.mt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editing the format of the filter names and putting them together in a set so that we won't have an issue later when filtering the matrixTable using difference()\n",
    "# create a set of the gnomAD qc filters (column names under \"sample filters\") - looks like: {'sex_aneuploidy', 'insert_size', ...} but not in a certain order (randomly ordered)\n",
    "all_sample_filters = set(mt['sample_filters']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # for renaming purposes\n",
    "\n",
    "# bad_sample_filters are filters that removed whole populations despite them passing all other gnomAD filters (mostly AFR and OCE popns)\n",
    "# remove \"fail_\" from the filter names and pick those out (9 filters) - if the filter name starts with 'fail_' then replace it with ''\n",
    "bad_sample_filters = {re.sub('fail_', '', x) for x in all_sample_filters if x.startswith('fail_')} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this filters to only variants that passed all gnomad QC or only failed filters in bad_sample_filters\n",
    "# 'qc_metrics_filters' is under 'sample_filters' and includes a set of all qc filters a particular sample failed \n",
    "# if a sample passed all gnomAD qc filters then the column entry for that sample under 'qc_metrics_filters' is an empty set\n",
    "# so this line goes through the 'qc_metrics_filters'column and sees if there are any samples that passed all the other qc filters except for the ones in the \"bad_sample_filters\" set (difference()) \n",
    "# if a sample has an empty set for the 'qc_metrics_filters' column or if it only failed the filters that are found in the bad_sample_filters set, then a value of zero is returned and we would keep that sample \n",
    "# if a sample failed any filters that are not in the \"bad_sample_filters\" set, then remove it\n",
    "mt_filt = mt.filter_cols(mt['sample_filters']['qc_metrics_filters'].difference(bad_sample_filters).length() == 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many were removed \n",
    "mt.count() # (211358784, 4151)\n",
    "mt_filt.count() # (211358784, 4017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the filtered matrixTable temporarily to a cloud bucket (took ~29 min to run)  \n",
    "mt_filt.checkpoint('gs://african-seq-data/hgdp_tgp/intersect_data_output.mt', overwrite = False, _read_if_exists = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read filtered mt back in \n",
    "mt_filt = hl.read_matrix_table('gs://african-seq-data/hgdp_tgp/intersect_data_output.mt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *ld_prune_filter* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run common variant statistics (quality control metrics) - more info (https://hail.is/docs/0.2/methods/genetics.html#hail.methods.variant_qc)  \n",
    "mt_var = hl.variant_qc(mt_filt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to get down to ~100-300k SNPs - might need to change values later accordingly  \n",
    "# AF: allele freq and call_rate: fraction of calls neither missing nor filtered\n",
    "# mt.variant_qc.AF[0] is referring to the first element of the list under that column\n",
    "mt_var_filt = mt_var.filter_rows((mt_var.variant_qc.AF[0] > 0.05) & (mt_var.variant_qc.AF[0] < 0.95) & (mt_var.variant_qc.call_rate > 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~14min to run \n",
    "mt_var_filt.count() # (6844706, 4017) - 6844706 snps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~76 min to run \n",
    "pruned = hl.ld_prune(mt_var_filt.GT, r2=0.1, bp_window_size=500000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data even further   \n",
    "mt_var_pru_filt = mt_var_filt.filter_rows(hl.is_defined(pruned[mt_var_filt.row_key])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the output as a temp file - make sure to save the file on this step b/c the pruning step takes a while to run\n",
    "# saving took ~22 min \n",
    "mt_var_pru_filt.write('gs://african-seq-data/hgdp_tgp/filtered_n_pruned_output.mt', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after saving the pruned file to the cloud, reading it back in for the next steps \n",
    "mt_var_pru_filt = hl.read_matrix_table('gs://african-seq-data/hgdp_tgp/filtered_n_pruned_output.mt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many snps are left after filtering and prunning? \n",
    "mt_var_pru_filt.count() # (255666, 4017) - 255666 snps \n",
    "# between ~100-300k so we proceed without any value adjustments  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *run_pc_relate*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a hail table is produced (~4-5min to run) \n",
    "relatedness_ht = hl.pc_relate(mt_var_pru_filt.GT, min_individual_maf=0.05, min_kinship=0.05, statistics='kin', k=20).key_by()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify related individuals in pairs to remove - returns a list of sample IDs (took ~13min to run)\n",
    "related_samples_to_remove = hl.maximal_independent_set(relatedness_ht.i, relatedness_ht.j, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sample IDs (col_key of the matrixTable), pick out the samples that are not found in 'related_samples_to_remove' (had 'False' values for the comparison)  \n",
    "# subset the matrixTable to those only \n",
    "mt_unrel = mt_var_pru_filt.filter_cols(hl.is_defined(related_samples_to_remove[mt_var_pru_filt.col_key]), keep=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same as above but this time for the samples with 'True' values (found in 'related_samples_to_remove')  \n",
    "mt_rel = mt_var_pru_filt.filter_cols(hl.is_defined(related_samples_to_remove[mt_var_pru_filt.col_key]), keep=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out mts of unrelated and related samples on to the cloud \n",
    "\n",
    "# unrelated mt\n",
    "mt_unrel.write('gs://african-seq-data/hgdp_tgp/unrel.mt', overwrite=False) \n",
    "\n",
    "# related mt \n",
    "mt_rel.write('gs://african-seq-data/hgdp_tgp/rel.mt', overwrite=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read saved mts back in \n",
    "\n",
    "# unrelated mt\n",
    "mt_unrel = hl.read_matrix_table('gs://african-seq-data/hgdp_tgp/unrel.mt') \n",
    "\n",
    "# related mt \n",
    "mt_rel = hl.read_matrix_table('gs://african-seq-data/hgdp_tgp/rel.mt') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code addition for subcontinental pca "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *run_pca* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca(mt: hl.MatrixTable, reg_name:str, out_prefix: str, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Runs PCA on a dataset\n",
    "    :param mt: dataset to run PCA on\n",
    "    :param reg_name: region name for saving output purposes\n",
    "    :param out_prefix: path for where to save the outputs\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    pca_evals, pca_scores, pca_loadings = hl.hwe_normalized_pca(mt.GT, k=20, compute_loadings=True)\n",
    "    pca_mt = mt.annotate_rows(pca_af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2)\n",
    "    pca_loadings = pca_loadings.annotate(pca_af=pca_mt.rows()[pca_loadings.key].pca_af)\n",
    "    pca_scores = pca_scores.transmute(**{f'PC{i}': pca_scores.scores[i - 1] for i in range(1, 21)})\n",
    "    \n",
    "    pca_scores.export(out_prefix + reg_name + '_scores.txt.bgz')  # save individual-level genetic region PCs\n",
    "    pca_loadings.write(out_prefix + reg_name + '_loadings.ht', overwrite)  # save PCA loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *project_individuals*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if running on GCP, need to add \"--packages gnomad\" when starting a cluster in order for the import to work  \n",
    "from gnomad.sample_qc.ancestry import *\n",
    "\n",
    "def project_individuals(pca_loadings, project_mt, reg_name:str, out_prefix: str, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Project samples into predefined PCA space\n",
    "    :param pca_loadings: existing PCA space - unrelated samples \n",
    "    :param project_mt: matrixTable of data to project - related samples \n",
    "    :param reg_name: region name for saving output purposes\n",
    "    :param project_prefix: path for where to save PCA projection outputs\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ht_projections = pc_project(project_mt, pca_loadings)  \n",
    "    ht_projections = ht_projections.transmute(**{f'PC{i}': ht_projections.scores[i - 1] for i in range(1, 21)}) \n",
    "    ht_projections.export(out_prefix + reg_name + '_projected_scores.txt.bgz') # save output \n",
    "    #return ht_projections # return to user  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a list of the genetic regions in the dataset \n",
    "regions = mt_unrel['hgdp_tgp_meta']['Genetic']['region'].collect()\n",
    "regions = list(dict.fromkeys(regions)) # 7 regions - ['EUR', 'AFR', 'AMR', 'EAS', 'CSA', 'OCE', 'MID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set argument values \n",
    "subcont_pca_prefix = 'gs://african-seq-data/hgdp_tgp/subcont_pca/subcont_pca_' # path for outputs \n",
    "overwrite = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'run_pca' function for each region  \n",
    "for i in regions:\n",
    "    subcont_unrel = mt_unrel.filter_cols(mt_unrel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the unrelateds per region\n",
    "    run_pca(subcont_unrel, i, subcont_pca_prefix, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'project_relateds' function for each region \n",
    "for i in regions:\n",
    "    loadings = hl.read_table(subcont_pca_prefix + i + '_loadings.ht') # for each region, read in the PCA loadings that were obtained from 'run_pca' function \n",
    "    subcont_rel = mt_rel.filter_cols(mt_rel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the unrelateds per region \n",
    "    project_individuals(loadings, subcont_rel, i, subcont_pca_prefix, overwrite) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After plotting the PCAs, a couple of outliers that needed to be removed were identified\n",
    "\n",
    "\n",
    "| s | Genetic region | Population | Note |\n",
    "| --- | --- | --- | -- |\n",
    "| NA20314 | AFR | ASW | Clusters with AMR in global PCA | \n",
    "| NA20299 | - | - | - |\n",
    "| HG01880 | - | - | - |\n",
    "| HG01881 | - | - | - |\n",
    "| HGDP00013 | - | - | - |\n",
    "| HGDP00150 | - | - | - |\n",
    "| HGDP00029 | - | - | - |\n",
    "| HGDP01298 | - | - | - |\n",
    "| HGDP00130 | CSA | Makrani | Closer to AFR than most CSA |\n",
    "| HGDP01303 | - | - | - |\n",
    "| LP6005443-DNA_B02 | - | - | - |\n",
    "| HGDP01300 | - | - | - |\n",
    "| HG01628 | - | - | - |\n",
    "| HG01629 | - | - | - |\n",
    "| HG01630 | - | - | - |\n",
    "| HG01694 | - | - | - |\n",
    "| HG01696 | - | - | - |\n",
    "| HGDP00621 | MID | Bedouin | Closer to AFR than most MID |\n",
    "| HGDP01270 | MID | Mozabite | Closer to AFR than most MID |\n",
    "| HGDP01271 | MID | Mozabite | Closer to AFR than most MID |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read back in the unrelated and related matrixTables to remove outliers and run pca \n",
    "mt_unrel_unfiltered = hl.read_matrix_table('gs://african-seq-data/hgdp_tgp/unrel.mt') \n",
    "mt_rel_unfiltered = hl.read_matrix_table('gs://african-seq-data/hgdp_tgp/rel.mt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the outliers file into a list\n",
    "with hl.utils.hadoop_open('gs://african-seq-data/hgdp_tgp/pca_outliers.txt') as file: \n",
    "    outliers = [line.rstrip('\\n') for line in file]\n",
    "    \n",
    "# capture and broadcast the list as an expression\n",
    "outliers_list = hl.literal(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers \n",
    "mt_unrel = mt_unrel_unfiltered.filter_cols(~outliers_list.contains(mt_unrel_unfiltered['s']))\n",
    "mt_rel = mt_rel_unfiltered.filter_cols(~outliers_list.contains(mt_rel_unfiltered['s']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrelated: Before filtering 3344 | After filtering 3327\n",
      "Related: Before filtering: 673 | After filtering 670\n",
      "Total outliers removed = 20\n"
     ]
    }
   ],
   "source": [
    "# sanity check \n",
    "print('Unrelated: Before filtering ' + str(mt_unrel_unfiltered.count()[1]) + ' | After filtering ' + str(mt_unrel.count()[1]))\n",
    "print('Related: Before filtering: ' + str(mt_rel_unfiltered.count()[1]) + ' | After filtering ' + str(mt_rel.count()[1]))\n",
    "\n",
    "num_outliers = (mt_unrel_unfiltered.count()[1] - mt_unrel.count()[1]) + (mt_rel_unfiltered.count()[1] - mt_rel.count()[1])\n",
    "print('Total outliers removed = ' + str(num_outliers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - The following steps are similar to the ones prior to removing the outliers except now we are using the updated unrelated & related dataset, and a new GC bucket path to save the outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a list of the genetic regions in the dataset - used the unrelated dataset since it had more samples  \n",
    "regions = mt_unrel['hgdp_tgp_meta']['Genetic']['region'].collect()\n",
    "regions = list(dict.fromkeys(regions)) # 7 regions - ['EUR', 'AFR', 'AMR', 'EAS', 'CSA', 'OCE', 'MID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set argument values \n",
    "subcont_pca_prefix = 'gs://african-seq-data/hgdp_tgp/subcont_pca_outliers_removed/subcont_pca_' # path for outputs \n",
    "overwrite = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'run_pca' function (located above) for each region - took roughly 25-30 min (notebook became slow)\n",
    "for i in regions:\n",
    "    subcont_unrel = mt_unrel.filter_cols(mt_unrel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the unrelateds per region\n",
    "    run_pca(subcont_unrel, i, subcont_pca_prefix, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'project_relateds' function (located above) for each region - took ~3min \n",
    "for i in regions:\n",
    "    loadings = hl.read_table(subcont_pca_prefix + i + '_loadings.ht') # for each region, read in the PCA loadings that were obtained from 'run_pca' function \n",
    "    subcont_rel = mt_rel.filter_cols(mt_rel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the unrelateds per region \n",
    "    project_individuals(loadings, subcont_rel, i, subcont_pca_prefix, overwrite) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
