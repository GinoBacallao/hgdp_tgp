{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-plane",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-bangladesh",
   "metadata": {},
   "source": [
    "# *setup_dataset* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the dataset Zan produced \n",
    "# two fields from Alicia’s metadata + Julia's sample QC metadata + variant QC metadata + Konrad’s densified mt\n",
    "mt = hl.read_matrix_table('gs://african-seq-data/hgdp_tgp/hgdp_tgp_dense_meta_filt.mt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# editing the format of the filter names and putting them together in a set so that we won't have an issue later when filtering the matrixTable using difference()\n",
    "# create a set of the gnomAD qc filters (column names under \"sample filters\") - looks like: {'sex_aneuploidy', 'insert_size', ...} but not in a certain order (randomly ordered)\n",
    "all_sample_filters = set(mt['sample_filters']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # for renaming purposes\n",
    "\n",
    "# bad_sample_filters are filters that removed whole populations despite them passing all other gnomAD filters (mostly AFR and OCE popns)\n",
    "# remove \"fail_\" from the filter names and pick those out (9 filters) - if the filter name starts with 'fail_' then replace it with ''\n",
    "bad_sample_filters = {re.sub('fail_', '', x) for x in all_sample_filters if x.startswith('fail_')} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerous-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this filters to only variants that passed all gnomad QC or only failed filters in bad_sample_filters\n",
    "# 'qc_metrics_filters' is under 'sample_filters' and includes a set of all qc filters a particular sample failed \n",
    "# if a sample passed all gnomAD qc filters then the column entry for that sample under 'qc_metrics_filters' is an empty set\n",
    "# so this line goes through the 'qc_metrics_filters'column and sees if there are any samples that passed all the other qc filters except for the ones in the \"bad_sample_filters\" set (difference()) \n",
    "# if a sample has an empty set for the 'qc_metrics_filters' column or if it only failed the filters that are found in the bad_sample_filters set, then a value of zero is returned and we would keep that sample \n",
    "# if a sample failed any filters that are not in the \"bad_sample_filters\" set, then remove it\n",
    "mt_filt = mt.filter_cols(mt['sample_filters']['qc_metrics_filters'].difference(bad_sample_filters).length() == 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many were removed \n",
    "mt.count() # (211358784, 4151)\n",
    "mt_filt.count() # (211358784, 4017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the filtered matrixTable temporarily to a cloud bucket (took ~29 min to run)  \n",
    "mt_filt.checkpoint('gs://african-seq-data/hgdp_tgp/intersect_data_output.mt', overwrite = False, _read_if_exists = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read filtered mt back in \n",
    "mt_filt = hl.read_matrix_table('gs://african-seq-data/hgdp_tgp/intersect_data_output.mt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-trunk",
   "metadata": {},
   "source": [
    "# *ld_prune_filter* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run common variant statistics (quality control metrics) - more info (https://hail.is/docs/0.2/methods/genetics.html#hail.methods.variant_qc)  \n",
    "mt_var = hl.variant_qc(mt_filt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-marina",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to get down to ~100-300k SNPs - might need to change values later accordingly  \n",
    "# AF: allele freq and call_rate: fraction of calls neither missing nor filtered\n",
    "# mt.variant_qc.AF[0] is referring to the first element of the list under that column\n",
    "mt_var_filt = mt_var.filter_rows((mt_var.variant_qc.AF[0] > 0.05) & (mt_var.variant_qc.AF[0] < 0.95) & (mt_var.variant_qc.call_rate > 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-chester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~14min to run \n",
    "mt_var_filt.count() # (6844706, 4017) - 6844706 snps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-april",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~76 min to run \n",
    "pruned = hl.ld_prune(mt_var_filt.GT, r2=0.1, bp_window_size=500000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data even further   \n",
    "mt_var_pru_filt = mt_var_filt.filter_rows(hl.is_defined(pruned[mt_var_filt.row_key])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the output as a temp file - make sure to save the file on this step b/c the pruning step takes a while to run\n",
    "# saving took ~22 min \n",
    "mt_var_pru_filt.write('gs://african-seq-data/hgdp_tgp/filtered_n_pruned_output.mt', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after saving the pruned file to the cloud, reading it back in for the next steps \n",
    "mt_var_pru_filt = hl.read_matrix_table('gs://african-seq-data/hgdp_tgp/filtered_n_pruned_output.mt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-ambassador",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many snps are left after filtering and prunning? \n",
    "mt_var_pru_filt.count() # (255666, 4017) - 255666 snps \n",
    "# between ~100-300k so we proceed without any value adjustments  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-surgeon",
   "metadata": {},
   "source": [
    "# *run_pc_relate*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a hail table is produced (~4-5min to run) \n",
    "relatedness_ht = hl.pc_relate(mt_var_pru_filt.GT, min_individual_maf=0.05, min_kinship=0.05, statistics='kin', k=20).key_by()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify related individuals in pairs to remove - returns a list of sample IDs (took ~13min to run)\n",
    "related_samples_to_remove = hl.maximal_independent_set(relatedness_ht.i, relatedness_ht.j, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-november",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sample IDs (col_key of the matrixTable), pick out the samples that are not found in 'related_samples_to_remove' (had 'False' values for the comparison)  \n",
    "# subset the matrixTable to those only \n",
    "mt_unrel = mt_var_pru_filt.filter_cols(hl.is_defined(related_samples_to_remove[mt_var_pru_filt.col_key]), keep=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same as above but this time for the samples with 'True' values (found in 'related_samples_to_remove')  \n",
    "mt_rel = mt_var_pru_filt.filter_cols(hl.is_defined(related_samples_to_remove[mt_var_pru_filt.col_key]), keep=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out mts of unrelated and related samples on to the cloud \n",
    "\n",
    "# unrelated mt\n",
    "mt_unrel.write('gs://african-seq-data/hgdp_tgp/unrel.mt', overwrite=False) \n",
    "\n",
    "# related mt \n",
    "mt_rel.write('gs://african-seq-data/hgdp_tgp/rel.mt', overwrite=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read saved mts back in \n",
    "\n",
    "# unrelated mt\n",
    "mt_unrel = hl.read_matrix_table('gs://african-seq-data/hgdp_tgp/unrel.mt') \n",
    "\n",
    "# related mt \n",
    "mt_rel = hl.read_matrix_table('gs://african-seq-data/hgdp_tgp/rel.mt') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-sound",
   "metadata": {},
   "source": [
    "# code addition for subcontinental pca "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "danish-junction",
   "metadata": {},
   "source": [
    "# *run_pca* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca(mt: hl.MatrixTable, reg_name:str, out_prefix: str, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Runs PCA on a dataset\n",
    "    :param mt: dataset to run PCA on\n",
    "    :param reg_name: region name for saving output purposes\n",
    "    :param out_prefix: path for where to save the outputs\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    pca_evals, pca_scores, pca_loadings = hl.hwe_normalized_pca(mt.GT, k=20, compute_loadings=True)\n",
    "    pca_mt = mt.annotate_rows(pca_af=hl.agg.mean(mt.GT.n_alt_alleles()) / 2)\n",
    "    pca_loadings = pca_loadings.annotate(pca_af=pca_mt.rows()[pca_loadings.key].pca_af)\n",
    "    pca_scores = pca_scores.transmute(**{f'PC{i}': pca_scores.scores[i - 1] for i in range(1, 21)})\n",
    "    \n",
    "    pca_scores.export(out_prefix + reg_name + '_scores.txt.bgz')  # save individual-level genetic region PCs\n",
    "    pca_loadings.write(out_prefix + reg_name + '_loadings.ht', overwrite)  # save PCA loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-wilson",
   "metadata": {},
   "source": [
    "# *project_individuals*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if running on GCP, need to add \"--packages gnomad\" when starting a cluster in order for the import to work  \n",
    "from gnomad.sample_qc.ancestry import *\n",
    "\n",
    "def project_individuals(pca_loadings, project_mt, reg_name:str, out_prefix: str, overwrite: bool = False):\n",
    "    \"\"\"\n",
    "    Project samples into predefined PCA space\n",
    "    :param pca_loadings: existing PCA space - unrelated samples \n",
    "    :param project_mt: matrixTable of data to project - related samples \n",
    "    :param reg_name: region name for saving output purposes\n",
    "    :param project_prefix: path for where to save PCA projection outputs\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    ht_projections = pc_project(project_mt, pca_loadings)  \n",
    "    ht_projections = ht_projections.transmute(**{f'PC{i}': ht_projections.scores[i - 1] for i in range(1, 21)}) \n",
    "    ht_projections.export(out_prefix + reg_name + '_projected_scores.txt.bgz') # save output \n",
    "    #return ht_projections # return to user  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a list of the genetic regions in the dataset \n",
    "regions = mt_unrel['hgdp_tgp_meta']['Genetic']['region'].collect()\n",
    "regions = list(dict.fromkeys(regions)) # 7 regions - ['EUR', 'AFR', 'AMR', 'EAS', 'CSA', 'OCE', 'MID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-selling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set argument values \n",
    "subcont_pca_prefix = 'gs://african-seq-data/hgdp_tgp/subcont_pca/subcont_pca_' # path for outputs \n",
    "overwrite = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'run_pca' function for each region  \n",
    "for i in regions:\n",
    "    subcont_unrel = mt_unrel.filter_cols(mt_unrel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the unrelateds per region\n",
    "    run_pca(subcont_unrel, i, subcont_pca_prefix, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'project_relateds' function for each region \n",
    "for i in regions:\n",
    "    loadings = hl.read_table(subcont_pca_prefix + i + '_loadings.ht') # for each region, read in the PCA loadings that were obtained from 'run_pca' function \n",
    "    subcont_rel = mt_rel.filter_cols(mt_rel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the unrelateds per region \n",
    "    project_individuals(loadings, subcont_rel, i, subcont_pca_prefix, overwrite) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-violence",
   "metadata": {},
   "source": [
    "## After plotting the PCAs, a couple of outliers that needed to be removed were identified\n",
    "\n",
    "\n",
    "| Genetic region | Population | s | Note |\n",
    "| --- | --- | --- | -- |\n",
    "| AFR | ASW | NA20314 | Clusters with AMR in global PCA | \n",
    "| CSA | Makrani | HGDP00130 | Closer to AFR than most CSA |\n",
    "| MID | Bedouin | HGDP00621 | Closer to AFR than most MID |\n",
    "| MID | Mozabite | HGDP01270 | Closer to AFR than most MID |\n",
    "| MID | Mozabite | HGDP01271 | Closer to AFR than most MID |\n",
    "\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read back in the unrelated matrixTable to remove the outliers \n",
    "mt_unrel_unfiltered = hl.read_matrix_table('gs://african-seq-data/hgdp_tgp/unrel.mt') \n",
    "\n",
    "# read back in related mt for pca projection \n",
    "mt_rel = hl.read_matrix_table('gs://african-seq-data/hgdp_tgp/rel.mt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outlier samples \n",
    "mt_unrel = mt_unrel_unfiltered.filter_cols((mt_unrel_unfiltered.s != 'NA20314') & (mt_unrel_unfiltered.s != 'HGDP00130') & (mt_unrel_unfiltered.s != 'HGDP00621') & (mt_unrel_unfiltered.s != 'HGDP01270') & (mt_unrel_unfiltered.s != 'HGDP01271'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "mt_unrel.count() # (255666, 3339)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-benefit",
   "metadata": {},
   "source": [
    "## - The following steps are similar to the ones prior to removing the outliers except now we are using the updated unrelated dataset and a new GC bucket path to save the outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain a list of the genetic regions in the dataset \n",
    "regions = mt_unrel['hgdp_tgp_meta']['Genetic']['region'].collect()\n",
    "regions = list(dict.fromkeys(regions)) # 7 regions - ['EUR', 'AFR', 'AMR', 'EAS', 'CSA', 'OCE', 'MID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set argument values \n",
    "subcont_pca_prefix = 'gs://african-seq-data/hgdp_tgp/subcont_pca_outliers_removed/subcont_pca_' # path for outputs \n",
    "overwrite = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'run_pca' function for each region - took ~25 min \n",
    "for i in regions:\n",
    "    subcont_unrel = mt_unrel.filter_cols(mt_unrel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the unrelateds per region\n",
    "    run_pca(subcont_unrel, i, subcont_pca_prefix, overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-method",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run 'project_relateds' function for each region - took ~3min \n",
    "for i in regions:\n",
    "    loadings = hl.read_table(subcont_pca_prefix + i + '_loadings.ht') # for each region, read in the PCA loadings that were obtained from 'run_pca' function \n",
    "    subcont_rel = mt_rel.filter_cols(mt_rel['hgdp_tgp_meta']['Genetic']['region'] == i)  # filter the unrelateds per region \n",
    "    project_individuals(loadings, subcont_rel, i, subcont_pca_prefix, overwrite) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}